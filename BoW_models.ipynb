{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BoW_models.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kH6GAPf8blU"
      },
      "source": [
        "# Bag of words models\n",
        "\n",
        "In this notebook we present the models that doesn't rely on the use of word embeddings. This part is composed of two main parts: the first part is made of retrieval based methods, while the second part is linear regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXrKOa7V88T2"
      },
      "source": [
        "**Retrieval-based approaches**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sm4KtDVD9MZf"
      },
      "source": [
        "# Mount the drive folder, don't execute in your experiments\n",
        "# import sys\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive/') \n",
        "# CWroot = \"drive/MyDrive/NLP_CW/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zgAiyCY97LE"
      },
      "source": [
        "# Execute only if not connected to the drive folder\n",
        "CWroot = \".\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFQB8L_z8aGB"
      },
      "source": [
        "# Imports and instals\n",
        "!pip install \"nltk==3.4.5\"\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
        "from collections import Counter\n",
        "import random\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7REtMra-IKI"
      },
      "source": [
        "# Load data\n",
        "train_df = pd.read_csv('%sdata/task-1/train.csv'%CWroot)\n",
        "test_df = pd.read_csv('%sdata/task-1/dev.csv'%CWroot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ19ftcg-I1c"
      },
      "source": [
        "\"\"\"\n",
        "Given a sentence with a token of the form <word/>, and a new_word, \n",
        "returns the preprocessed list of words with the new_word in place.\n",
        "\"\"\"\n",
        "def replace(sentence, new_word):\n",
        "    l = sentence.split(' ')\n",
        "    l = [word if not (\"<\" in word and \"/>\" in word) else new_word for word in l]\n",
        "    sentence = ' '.join(l).lower().strip()\n",
        "    sentence = re.sub(r'[^\\w\\s]','',sentence)\n",
        "    tokens = word_tokenize(sentence)\n",
        "    result = [i for i in tokens if not i in ENGLISH_STOP_WORDS]\n",
        "\n",
        "    return result"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTTgf4mt-9K5"
      },
      "source": [
        "# Changing format for convenience, train and test are lists of tuples of\n",
        "# the form (sentence, score)\n",
        "train = []\n",
        "\n",
        "for sample in train_df.iloc:\n",
        "    sentence = replace(sample['original'], sample['edit'])\n",
        "    score = sample['meanGrade']\n",
        "    train.append((sentence, score))\n",
        "\n",
        "test = []\n",
        "\n",
        "for sample in test_df.iloc:\n",
        "    sentence = replace(sample['original'], sample['edit'])\n",
        "    score = sample['meanGrade']\n",
        "    test.append((sentence, score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ut7R6cxwAAc9"
      },
      "source": [
        "# Splitting train and dev set\n",
        "random.shuffle(train)\n",
        "\n",
        "train = train[:int(len(train)*0.9)]\n",
        "dev = train[int(len(train)*0.9):]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCGF14zU_PEf"
      },
      "source": [
        "\"\"\"\n",
        "Given two headlines a and b, the cosine similarity between their \n",
        "BoW representations is returned\n",
        "\"\"\"\n",
        "def cosine_similarity(a, b):\n",
        "    # count word occurrences\n",
        "    a_vals = Counter(a)\n",
        "    b_vals = Counter(b)\n",
        "\n",
        "    # convert to word-vectors\n",
        "    words = list(a_vals.keys() | b_vals.keys())\n",
        "    a_vect = [a_vals.get(word, 0) for word in words] \n",
        "    b_vect = [b_vals.get(word, 0) for word in words] \n",
        "\n",
        "    # find cosine\n",
        "    len_a = sum(av * av for av in a_vect) ** 0.5 \n",
        "    len_b = sum(bv * bv for bv in b_vect) ** 0.5 \n",
        "    dot = sum(av * bv for av, bv in zip(a_vect, b_vect)) \n",
        "    cosine = dot / (len_a * len_b) \n",
        "    return cosine"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQ0Fsgw__e4A"
      },
      "source": [
        "\"\"\"\n",
        "Iterates through the training set for finding the N headlines that are\n",
        "most similar by using cosine_similarity to the given sentence. \n",
        "Computes and returns the mean of the associated mean scores.\n",
        "\"\"\"\n",
        "def get_score(sentence, N):\n",
        "    current_best_similarities = [(-1, 0) for _ in range(N)]\n",
        "    for sentence_retrieved, score in train:\n",
        "        new_sim_score = cosine_similarity(sentence, sentence_retrieved)\n",
        "        for k in range(N):\n",
        "            if new_sim_score > current_best_similarities[k][0]:\n",
        "                current_best_similarities[k] = (new_sim_score, score)\n",
        "                current_best_similarities.sort(key = lambda x: x[0], )\n",
        "                break\n",
        "\n",
        "    sum = 0\n",
        "\n",
        "    for i in range(N):\n",
        "        sum += current_best_similarities[i][1]\n",
        "\n",
        "\n",
        "    return sum/N"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgtrdm_2ByfX"
      },
      "source": [
        "\"\"\"\n",
        "Iterates through the training set for finding the N headlines that are\n",
        "most similar by using BLEU as measure of similarity to the given sentence. \n",
        "Computes and returns the mean of the associated mean scores.\n",
        "\"\"\"\n",
        "def get_score_by_bleu(sentence, N):\n",
        "    current_best_similarities = [(-1, 0) for _ in range(N)]\n",
        "    for sentence_retrieved, score in train[:int(len(train)*0.9)]:\n",
        "        new_sim_score = sentence_bleu([sentence_retrieved], sentence)\n",
        "        for k in range(N):\n",
        "            if new_sim_score > current_best_similarities[k][0]:\n",
        "                current_best_similarities[k] = (new_sim_score, score)\n",
        "                current_best_similarities.sort(key = lambda x: x[0], )\n",
        "                break\n",
        "\n",
        "    sum = 0\n",
        "\n",
        "    for i in range(N):\n",
        "        sum += current_best_similarities[i][1]\n",
        "\n",
        "\n",
        "    return sum/N"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqqG1I04CEtt"
      },
      "source": [
        "\"\"\"\n",
        "Iterates through the training set for finding the N headlines that are\n",
        "most similar by using METEOR as measure of similarity to the given sentence. \n",
        "Computes and returns the mean of the associated mean scores.\n",
        "\"\"\"\n",
        "def get_score_by_meteor(sentence, N):\n",
        "    current_best_similarities = [(-1, 0) for _ in range(N)]\n",
        "    for sentence_retrieved, score in train[:int(len(train)*0.9)]:\n",
        "        new_sim_score = meteor_score([' '.join(sentence_retrieved)], sentence)\n",
        "        for k in range(N):\n",
        "            if new_sim_score > current_best_similarities[k][0]:\n",
        "                current_best_similarities[k] = (new_sim_score, score)\n",
        "                current_best_similarities.sort(key = lambda x: x[0], )\n",
        "                break\n",
        "\n",
        "    sum = 0\n",
        "\n",
        "    for i in range(N):\n",
        "        sum += current_best_similarities[i][1]\n",
        "\n",
        "\n",
        "    return sum/N"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1n_nCFsAp7U"
      },
      "source": [
        "# Computes the root mean square error between the predictions and the targets\n",
        "def rmse(prediction, target):\n",
        "    return mean_squared_error(target, prediction, squared=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uljVhiJ_A2j_"
      },
      "source": [
        "total_rmse = 0\n",
        "N = 5\n",
        "\n",
        "rmses_cos = []\n",
        "rmses_bleu = []\n",
        "rmses_meteor = []\n",
        "\n",
        "predictions = []\n",
        "scores = []\n",
        "\n",
        "for N in range(1,6):\n",
        "    # Computing with METEOR\n",
        "    total_mse = 0\n",
        "    print(\"=\"*50)\n",
        "    print(\"Experiments with K=\"+str(N))\n",
        "    for i, (sentence, score) in enumerate(train[int(len(train)*0.9):]):\n",
        "        pred_score = get_score_by_meteor(' '.join(sentence), N)\n",
        "        predictions.append(pred_score)\n",
        "        scores.append(score)\n",
        "\n",
        "    res = rmse(predictions, scores)\n",
        "\n",
        "    rmses_meteor.append(res)\n",
        "\n",
        "    print(\"METEOR part completed.\")\n",
        "    \n",
        "    # Computing with Cosine Similarity\n",
        "    total_mse = 0\n",
        "    for i, (sentence, score) in enumerate(train[int(len(train)*0.9):]):\n",
        "        pred_score = get_score(sentence, N)\n",
        "        predictions.append(pred_score)\n",
        "        scores.append(score)\n",
        "\n",
        "        if i % 500 == 0 and i != 0:\n",
        "            print(f\"Tested {i}/{len(train[int(len(train)*0.9):])}\")\n",
        "\n",
        "    res = rmse(predictions, scores)\n",
        "\n",
        "    rmses_cos.append(res)\n",
        "\n",
        "    print(\"Cosine Similarity part completed.\")\n",
        "\n",
        "    # Computing with BLEU\n",
        "    total_mse = 0\n",
        "    for i, (sentence, score) in enumerate(train[int(len(train)*0.9):]):\n",
        "        pred_score = get_score_by_bleu(sentence, N)\n",
        "        predictions.append(pred_score)\n",
        "        scores.append(score)\n",
        "\n",
        "        if i % 500 == 0 and i != 0:\n",
        "            print(f\"Tested {i}/{len(train[int(len(train)*0.9):])}\")\n",
        "\n",
        "    res = rmse(predictions, scores)\n",
        "\n",
        "    rmses_bleu.append(res)\n",
        "\n",
        "    print(\"BLEU part completed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZrpZUZpBhq1"
      },
      "source": [
        "plt.plot([1,2,3,4,5], rmses_cos, label=\"Cosine similarity\")\n",
        "plt.plot([1,2,3,4,5], rmses_bleu, label=\"BLEU\")\n",
        "plt.plot([1,2,3,4,5], rmses_meteor, label=\"METEOR\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WjRU3yjCuyQ"
      },
      "source": [
        "**Linear regression with Bag of Words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1przLFiC8Vd"
      },
      "source": [
        "#imports\n",
        "from collections import Counter\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FghUsztC6we"
      },
      "source": [
        "all_sentences = []\n",
        "\n",
        "# Concatenate all sentences\n",
        "for sentence, _ in train:\n",
        "    all_sentences = [*all_sentences, *sentence]\n",
        "\n",
        "# Make counter for all_sentences\n",
        "vals = Counter(all_sentences)\n",
        "\n",
        "words = []\n",
        "\n",
        "# Get the list of the 300 most common words\n",
        "for word, occ in vals.most_common(300):\n",
        "    words.append(word)\n",
        "\n",
        "# For each sample in the training set, build its BoW\n",
        "# representation and store it in words_bow\n",
        "words_bow = []\n",
        "for i in range(len(train)):\n",
        "    word_bow = [0 for w in words]\n",
        "    sentence = train[i][0]\n",
        "    for word in sentence:\n",
        "        if word in words:\n",
        "            idx = words.index(word)\n",
        "            word_bow[idx] += 1\n",
        "\n",
        "    words_bow.append(word_bow)\n",
        "\n",
        "# Does the same for the dev set\n",
        "words_bow_dev = []\n",
        "for i in range(len(dev)):\n",
        "    word_bow = [0 for w in words]\n",
        "    sentence = dev[i][0]\n",
        "    for word in sentence:\n",
        "        if word in words:\n",
        "            idx = words.index(word)\n",
        "            word_bow[idx] += 1\n",
        "\n",
        "    words_bow_dev.append(word_bow)\n",
        "\n",
        "# Does the same for the test set\n",
        "words_bow_test = []\n",
        "for i in range(len(test)):\n",
        "    word_bow = [0 for w in words]\n",
        "    sentence = test[i][0]\n",
        "    for word in sentence:\n",
        "        if word in words:\n",
        "            idx = words.index(word)\n",
        "            word_bow[idx] += 1\n",
        "\n",
        "    words_bow_test.append(word_bow)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mUnbSFqDaNA"
      },
      "source": [
        "# Function for training the model\n",
        "def train_linear_regression(features, label):\n",
        "    print (\"Training the linear regression model...\")\n",
        "    ml_model = LinearRegression()\n",
        "    ml_model.fit(features, label)\n",
        "    print ('Finished')\n",
        "\n",
        "    return ml_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEbihQKKDiau"
      },
      "source": [
        "# Train the model\n",
        "ml_model = train_linear_regression(words_bow, [y for x,y in train])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZI485RvDnua"
      },
      "source": [
        "# Measure performance over dev set\n",
        "res = ml_model.predict(words_bow_dev)\n",
        "print(mean_squared_error([y for x,y in dev], res, squared=False))\n",
        "print(r2_score([y for x,y in dev], res))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8c5CpLWEf-f"
      },
      "source": [
        "# Plots the distribution of outputs of this model\n",
        "plt.hist(x=res, bins=10, alpha=0.7, rwidth=0.85, density=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLr0UkIRGPGH"
      },
      "source": [
        "# Retrain model by using train+dev sets\n",
        "headlines = [*words_bow, *words_bow_dev]\n",
        "y_train = [y for x,y in train]\n",
        "y_dev = [y for x,y in dev]\n",
        "y = [*y_train, *y_dev]\n",
        "\n",
        "ml_model = train_linear_regression(headlines, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeonMmdSFq95"
      },
      "source": [
        "# Measure performance over test set\n",
        "res = ml_model.predict(words_bow_test)\n",
        "print(mean_squared_error([y for x,y in test], res, squared=False))\n",
        "print(r2_score([y for x,y in test], res))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5pX606eFyQ-"
      },
      "source": [
        "# Plots the distribution of outputs of this model\n",
        "plt.hist(x=res, bins=10, alpha=0.7, rwidth=0.85, density=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}